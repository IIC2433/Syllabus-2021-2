{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "789c6d0d",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "Los autoencoders son redes neuronales que aprenden sobre un mismo set de datos, y pueden usarse para reducir su dimensionalidad. Esta semana seguimos trabajando con el dataset de MNIST.  Vamos a ver como aprender y cómo usar estas redes, y el ejercicio abierto es que veas la capacidad de reducir dimensionalidad para visualizar.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5f21cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1, as_frame=True)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b7fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Tomamos el dataset y lo dividimos en X (los pixeles) e y (el número que es). \n",
    "#Recordemos que las imágenes son 28x28 = 784 pixeles\n",
    "X, y = mnist['data'], mnist['target']\n",
    "X_sample = X[60000:]\n",
    "y_sample = y[60000:]\n",
    "\n",
    "### Para dividir en train  - test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.3, random_state=23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75260e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c9cb810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Si bien los datos no están completamente balanceados, hay una buena mezcla de cada número en el dataset.\n",
    "y_sample.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "203d29bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 784 artists>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Había quedado pendiente: un plot de la varianza de cada dimensión\n",
    "plt.bar(np.arange(len(X_sample.columns)),X_sample.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45ea917",
   "metadata": {},
   "source": [
    "Un aspecto bien importante, es que las redes autoencoders suelen trabajar mejor cuando podemos estandarizar los datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a509c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "### También vamos a estandarizar los datos en X\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_std = sc.fit_transform(X_sample)\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5afab2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -1.00005000e-02, -1.00005000e-02, -1.00005000e-02,\n",
       "       -1.39737990e-02, -1.89315247e-02, -2.31843010e-02, -3.60728861e-02,\n",
       "       -3.92619154e-02, -3.80269994e-02, -3.90143887e-02, -3.46046778e-02,\n",
       "       -2.57765396e-02, -2.09733754e-02, -2.17809993e-02, -1.44984527e-02,\n",
       "       -1.18807892e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -1.78081425e-02, -2.32058779e-02, -2.98662898e-02,\n",
       "       -4.14395151e-02, -5.86512813e-02, -8.12643979e-02, -1.05997038e-01,\n",
       "       -1.21704878e-01, -1.34457288e-01, -1.39756261e-01, -1.41562422e-01,\n",
       "       -1.35229133e-01, -1.20246727e-01, -1.04490087e-01, -8.70044931e-02,\n",
       "       -7.16699334e-02, -4.85892545e-02, -3.24260775e-02, -2.16926329e-02,\n",
       "       -1.00005000e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -1.32956624e-02, -2.25936238e-02, -3.83702224e-02, -5.98206019e-02,\n",
       "       -8.42014426e-02, -1.18390816e-01, -1.54266827e-01, -1.88282524e-01,\n",
       "       -2.19803054e-01, -2.42936317e-01, -2.55020324e-01, -2.59481423e-01,\n",
       "       -2.49404582e-01, -2.26727106e-01, -2.00418885e-01, -1.67161170e-01,\n",
       "       -1.34317009e-01, -9.58717755e-02, -7.36565245e-02, -5.03983075e-02,\n",
       "       -2.69783475e-02, -1.68919000e-02, -1.00005000e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -1.47795885e-02,\n",
       "       -2.51221010e-02, -3.81226487e-02, -7.86317321e-02, -1.19593671e-01,\n",
       "       -1.65704529e-01, -2.28814281e-01, -2.88620224e-01, -3.54491034e-01,\n",
       "       -4.21140618e-01, -4.80243669e-01, -5.27064646e-01, -5.40807419e-01,\n",
       "       -5.21388017e-01, -4.74446021e-01, -4.03948632e-01, -3.36571539e-01,\n",
       "       -2.71580657e-01, -2.06667410e-01, -1.54539645e-01, -1.08856709e-01,\n",
       "       -6.77589146e-02, -3.40327281e-02, -2.15091205e-02,  0.00000000e+00,\n",
       "        0.00000000e+00, -1.00005000e-02, -1.07381289e-02, -2.60253876e-02,\n",
       "       -5.70600482e-02, -9.14378767e-02, -1.43000013e-01, -1.99005834e-01,\n",
       "       -2.66034404e-01, -3.53401549e-01, -4.50251488e-01, -5.51598332e-01,\n",
       "       -6.47939202e-01, -7.43171364e-01, -8.18162561e-01, -8.51073275e-01,\n",
       "       -8.31121680e-01, -7.63764496e-01, -6.59992784e-01, -5.47527626e-01,\n",
       "       -4.39376979e-01, -3.35576590e-01, -2.54856553e-01, -1.83933732e-01,\n",
       "       -1.26755715e-01, -7.06477667e-02, -3.88818206e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -1.34176155e-02, -3.90612132e-02,\n",
       "       -8.73974922e-02, -1.33107017e-01, -1.94532142e-01, -2.74786330e-01,\n",
       "       -3.69886454e-01, -4.82920333e-01, -6.05294063e-01, -7.35621386e-01,\n",
       "       -8.69509827e-01, -9.89564738e-01, -1.09132506e+00, -1.13182948e+00,\n",
       "       -1.09408349e+00, -9.96373436e-01, -8.68781173e-01, -7.17778845e-01,\n",
       "       -5.70649327e-01, -4.39021868e-01, -3.26889344e-01, -2.35934504e-01,\n",
       "       -1.67697996e-01, -9.95100269e-02, -4.79392976e-02, -1.87851186e-02,\n",
       "        0.00000000e+00, -1.17322667e-02, -2.88274493e-02, -6.46532861e-02,\n",
       "       -1.18956716e-01, -1.77837580e-01,  1.53795878e+00,  2.57176245e+00,\n",
       "        1.53212043e+00,  1.00392168e+00, -1.79355647e-01, -5.91732991e-01,\n",
       "       -1.05273662e+00, -1.15378689e+00, -1.22142979e+00, -1.23881560e+00,\n",
       "       -1.21321586e+00, -1.14302847e+00, -1.02018313e+00, -8.57098743e-01,\n",
       "       -6.76706697e-01, -5.16203262e-01, -3.79287244e-01, -2.71402545e-01,\n",
       "       -1.89934521e-01, -1.19940614e-01, -5.56340911e-02, -1.45752163e-02,\n",
       "        0.00000000e+00, -2.06611389e-02, -4.37166621e-02, -8.08756237e-02,\n",
       "       -1.40488164e-01, -2.07699245e-01,  3.77477260e+00,  3.14033146e+00,\n",
       "        2.28939169e+00,  1.76127332e+00,  1.43185420e+00,  1.13131350e+00,\n",
       "        6.79164893e-01,  6.65484747e-01,  6.66043389e-01,  6.80680095e-01,\n",
       "        6.77305174e-01,  6.65508286e-01,  7.21340316e-01,  8.83661589e-01,\n",
       "        9.17518690e-01,  2.82541074e-02, -4.01002939e-01, -2.83099723e-01,\n",
       "       -1.94831338e-01, -1.23075256e-01, -6.66126860e-02, -1.61462821e-02,\n",
       "       -1.12546885e-02, -2.93918605e-02, -4.84646663e-02, -9.31783260e-02,\n",
       "       -1.46682925e-01, -2.18121209e-01,  8.30460131e-01,  1.04725853e+00,\n",
       "        1.47086928e-01,  2.59684517e-01,  4.95679969e-01,  9.98953721e-01,\n",
       "        1.29535061e+00,  1.12204782e+00,  1.41528197e+00,  1.42599520e+00,\n",
       "        1.36416372e+00,  1.22805443e+00,  1.03395727e+00,  1.40874227e+00,\n",
       "        1.73166837e+00,  1.00260058e+00, -4.01823716e-01, -2.75049233e-01,\n",
       "       -1.81713744e-01, -1.07567122e-01, -5.66041118e-02, -1.89159236e-02,\n",
       "       -1.21427928e-02, -2.43168731e-02, -5.02703770e-02, -8.87358114e-02,\n",
       "       -1.38806025e-01, -2.12706019e-01, -3.21729999e-01, -4.62313723e-01,\n",
       "       -6.52442841e-01, -8.45524923e-01, -9.61258323e-01, -7.93125052e-01,\n",
       "       -2.26359955e-01, -6.40468216e-01, -1.23720090e-01, -1.67157468e-01,\n",
       "       -2.55843161e-01, -4.41448335e-01, -7.92766628e-01,  1.30597044e+00,\n",
       "        1.81460411e+00,  6.91054579e-01, -3.83665051e-01, -2.63105130e-01,\n",
       "       -1.66473946e-01, -7.99663431e-02, -4.55007946e-02, -1.95541446e-02,\n",
       "       -1.00005000e-02, -1.86206584e-02, -4.14986832e-02, -7.22615997e-02,\n",
       "       -1.23238725e-01, -2.12256343e-01, -3.31309824e-01, -4.91126078e-01,\n",
       "       -6.87704902e-01, -8.62602670e-01, -9.39124713e-01, -8.69991467e-01,\n",
       "       -7.58168797e-01, -7.22198511e-01, -7.39826964e-01, -8.09980626e-01,\n",
       "       -9.11188613e-01, -1.00032001e+00, -2.21550751e-01,  1.53134484e+00,\n",
       "        1.47605194e+00, -2.73150738e-01, -3.63157263e-01, -2.52975575e-01,\n",
       "       -1.57152039e-01, -6.52009258e-02, -3.35283586e-02, -1.24209728e-02,\n",
       "        0.00000000e+00, -1.48492790e-02, -3.29699917e-02, -6.01451792e-02,\n",
       "       -1.18353377e-01, -2.19271688e-01, -3.54392407e-01, -5.23006773e-01,\n",
       "       -7.15682870e-01, -8.62626101e-01, -9.05242890e-01, -8.31592288e-01,\n",
       "       -7.51312636e-01, -7.62948163e-01, -8.25877849e-01, -9.30232292e-01,\n",
       "       -1.04727288e+00, -8.79016953e-01,  1.11455708e+00,  1.61660969e+00,\n",
       "        2.64000765e-01, -4.64282235e-01, -3.54907482e-01, -2.56014147e-01,\n",
       "       -1.58427696e-01, -6.20647188e-02, -2.42921899e-02,  0.00000000e+00,\n",
       "        0.00000000e+00, -1.17874599e-02, -2.52632841e-02, -5.02423656e-02,\n",
       "       -1.15068847e-01, -2.35195531e-01, -3.77531303e-01, -5.47311188e-01,\n",
       "       -7.23069536e-01, -8.48981953e-01, -8.78897369e-01, -8.26469482e-01,\n",
       "       -7.95496372e-01, -8.83536617e-01, -9.94814123e-01, -1.13364619e+00,\n",
       "       -1.20871511e+00,  5.60198157e-05,  1.28700658e+00,  1.50082995e+00,\n",
       "       -1.22561277e-01, -4.62110102e-01, -3.60151562e-01, -2.63898374e-01,\n",
       "       -1.66295096e-01, -5.68635009e-02, -1.05441394e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -1.66367790e-02, -4.23254862e-02,\n",
       "       -1.19931644e-01, -2.52550583e-01, -3.91916340e-01, -5.56171069e-01,\n",
       "       -7.17849905e-01, -8.29516019e-01, -8.54549188e-01, -8.45989670e-01,\n",
       "       -8.89246054e-01, -1.03761315e+00, -1.16457617e+00, -1.30025654e+00,\n",
       "       -7.40699086e-01,  1.05188993e+00,  1.30369880e+00, -1.63440609e-01,\n",
       "       -5.90584640e-01, -4.74233049e-01, -3.68789557e-01, -2.74082099e-01,\n",
       "       -1.74264813e-01, -6.96188843e-02, -1.80031510e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -1.68610568e-02, -4.51688568e-02,\n",
       "       -1.31668459e-01, -2.67838929e-01, -3.98906806e-01, -5.48202377e-01,\n",
       "       -6.90077015e-01, -7.89823563e-01, -8.31599129e-01, -8.61314493e-01,\n",
       "       -9.56815660e-01, -1.11036634e+00, -1.22743073e+00, -1.31006468e+00,\n",
       "       -2.57368600e-02,  1.14239899e+00,  7.61423491e-01, -7.06825874e-01,\n",
       "       -6.08999426e-01, -4.92457882e-01, -3.80502867e-01, -2.79282191e-01,\n",
       "       -1.73984018e-01, -7.67235054e-02, -1.95871373e-02, -1.00005000e-02,\n",
       "        0.00000000e+00, -1.00005000e-02, -2.48178080e-02, -5.52275065e-02,\n",
       "       -1.48243512e-01, -2.83202341e-01, -4.02212500e-01, -5.34598048e-01,\n",
       "       -6.56007943e-01, -7.38083794e-01, -7.81657503e-01, -8.24620535e-01,\n",
       "       -9.18824463e-01, -1.04078449e+00, -1.13391454e+00, -1.09212795e+00,\n",
       "        7.05920310e-01,  1.17679031e+00, -3.73781820e-01, -7.58547572e-01,\n",
       "       -6.28680640e-01, -5.01492113e-01, -3.81043892e-01, -2.70505206e-01,\n",
       "       -1.68251255e-01, -7.84168728e-02, -2.27999680e-02, -1.57856413e-02,\n",
       "        0.00000000e+00,  0.00000000e+00, -2.69850288e-02, -6.76999793e-02,\n",
       "       -1.67498207e-01, -2.98089736e-01, -4.11096027e-01, -5.22810883e-01,\n",
       "       -6.25838621e-01, -6.93423683e-01, -7.31704263e-01, -7.67086709e-01,\n",
       "       -8.29980030e-01, -9.21590434e-01, -1.00562716e+00,  7.79492952e-02,\n",
       "        1.22959017e+00,  6.36500653e-01, -9.01400043e-01, -7.69630793e-01,\n",
       "       -6.35363773e-01, -4.94618472e-01, -3.69117095e-01, -2.55794246e-01,\n",
       "       -1.56732083e-01, -7.83809414e-02, -2.67109338e-02, -1.48726634e-02,\n",
       "        0.00000000e+00, -1.00005000e-02, -3.48385687e-02, -8.69311199e-02,\n",
       "       -1.85622432e-01, -3.11777198e-01, -4.27690033e-01, -5.30457702e-01,\n",
       "       -6.12837575e-01, -6.69073252e-01, -7.06628103e-01, -7.37178903e-01,\n",
       "       -7.79583917e-01, -8.66698428e-01, -2.88157768e-01,  1.21930590e+00,\n",
       "        1.10500698e+00, -5.04139890e-01, -9.09137779e-01, -7.74520432e-01,\n",
       "       -6.19405771e-01, -4.72096102e-01, -3.44822207e-01, -2.35626373e-01,\n",
       "       -1.44455008e-01, -7.69092863e-02, -2.86146987e-02, -1.00005000e-02,\n",
       "        0.00000000e+00, -1.00005000e-02, -3.42628198e-02, -1.01174053e-01,\n",
       "       -1.95711272e-01, -3.24606261e-01, -4.42716711e-01, -5.45960978e-01,\n",
       "       -6.37281741e-01, -7.03742928e-01, -7.53441795e-01, -7.88772419e-01,\n",
       "       -8.29773267e-01, -7.45526297e-01,  9.49893727e-01,  1.18293215e+00,\n",
       "        3.85795002e-01, -1.02329900e+00, -8.98728840e-01, -7.36858006e-01,\n",
       "       -5.75258663e-01, -4.30322485e-01, -3.09120250e-01, -2.09889823e-01,\n",
       "       -1.31895170e-01, -7.31506415e-02, -2.76674735e-02, -1.00005000e-02,\n",
       "        0.00000000e+00, -1.00005000e-02, -4.00234981e-02, -1.07093740e-01,\n",
       "       -1.94645695e-01, -3.16981297e-01, -4.40895564e-01, -5.60086039e-01,\n",
       "       -6.67605659e-01, -7.63806998e-01, -8.43535003e-01, -9.03604039e-01,\n",
       "       -9.38010529e-01,  7.63887624e-01,  1.12176928e+00,  7.84111000e-01,\n",
       "       -8.18046093e-01, -9.91046672e-01, -8.28340182e-01, -6.52780006e-01,\n",
       "       -4.95325185e-01, -3.64891317e-01, -2.61772085e-01, -1.75298870e-01,\n",
       "       -1.12966586e-01, -6.17374486e-02, -2.70715466e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -4.06825662e-02, -9.78606438e-02,\n",
       "       -1.77848987e-01, -2.87783481e-01, -4.12614752e-01, -5.43271605e-01,\n",
       "       -6.71018812e-01, -7.98159188e-01, -9.16686263e-01, -1.02499517e+00,\n",
       "       -7.73682132e-01,  1.09355574e+00,  1.05041156e+00, -4.98209852e-01,\n",
       "       -1.05256459e+00, -8.70980804e-01, -6.88431167e-01, -5.23166414e-01,\n",
       "       -3.91308572e-01, -2.82035183e-01, -1.99071147e-01, -1.36525170e-01,\n",
       "       -8.93688913e-02, -4.13170860e-02, -1.68508310e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -2.83386899e-02, -7.65120563e-02,\n",
       "       -1.41969555e-01, -2.32658498e-01, -3.41261378e-01, -4.69723228e-01,\n",
       "       -6.06194512e-01, -7.47366354e-01, -8.80786554e-01, -7.29389144e-01,\n",
       "        8.95224865e-01,  1.11943124e+00, -1.05438374e-01, -1.00783177e+00,\n",
       "       -8.59696548e-01, -6.83890026e-01, -5.31181637e-01, -3.95889778e-01,\n",
       "       -2.89956123e-01, -2.03267966e-01, -1.42951450e-01, -9.63532989e-02,\n",
       "       -6.43914026e-02, -3.37070214e-02, -1.11853003e-02,  0.00000000e+00,\n",
       "        0.00000000e+00, -1.00005000e-02, -1.51722732e-02, -4.80051146e-02,\n",
       "       -9.51161616e-02, -1.60643556e-01, -2.45453283e-01, -3.53245922e-01,\n",
       "       -4.74265429e-01, -5.98667391e-01, -7.29305101e-01,  3.89322873e-01,\n",
       "        1.38694264e+00,  1.37486731e+00, -4.03963644e-01, -7.74445930e-01,\n",
       "       -6.38730244e-01, -5.02999283e-01, -3.87339921e-01, -2.79971294e-01,\n",
       "       -1.98381814e-01, -1.35822721e-01, -9.65383286e-02, -6.33365644e-02,\n",
       "       -4.27549534e-02, -2.57581657e-02, -1.00005000e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -2.37543896e-02,\n",
       "       -5.22032466e-02, -8.58749627e-02, -1.40703979e-01, -2.08515621e-01,\n",
       "       -2.90149335e-01, -3.68567087e-01,  3.34201602e-01,  2.33307288e+00,\n",
       "        2.27286258e+00,  2.23777229e+00,  4.12218057e-02, -4.94890333e-01,\n",
       "       -4.22342015e-01, -3.39048837e-01, -2.57069088e-01, -1.85534152e-01,\n",
       "       -1.36577185e-01, -8.60242391e-02, -5.78259874e-02, -3.36364160e-02,\n",
       "       -1.81122384e-02, -1.00005000e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -1.36274661e-02,\n",
       "       -2.85803164e-02, -4.74793553e-02, -7.79785591e-02, -1.18532172e-01,\n",
       "       -1.67201555e-01, -2.14787719e-01,  2.22171299e+00,  4.30500754e+00,\n",
       "        4.03125111e+00,  3.36505818e+00,  3.79953648e-01, -2.84269948e-01,\n",
       "       -2.47694588e-01, -2.05869945e-01, -1.55925102e-01, -1.16435448e-01,\n",
       "       -8.57647974e-02, -5.46508166e-02, -4.01800073e-02, -2.37589970e-02,\n",
       "       -1.65780693e-02, -1.00005000e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -1.15748833e-02, -2.84271584e-02, -5.06655656e-02, -7.40332846e-02,\n",
       "       -1.00455604e-01, -1.24744578e-01,  4.17363552e+00,  7.81243004e+00,\n",
       "        5.78969790e+00,  3.22149281e-01, -1.81506609e-01, -1.60333393e-01,\n",
       "       -1.39182079e-01, -1.18875455e-01, -8.73316648e-02, -7.00227708e-02,\n",
       "       -5.40690537e-02, -3.84297037e-02, -2.65616274e-02, -1.61844507e-02,\n",
       "       -1.19683967e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -1.32918601e-02, -1.59980455e-02,\n",
       "       -2.07236291e-02, -2.66997366e-02, -2.84703819e-02, -3.43035092e-02,\n",
       "       -4.10336906e-02, -4.88886427e-02, -5.48357917e-02, -5.51988782e-02,\n",
       "       -4.69971082e-02, -3.88769026e-02, -3.16010302e-02, -2.85226846e-02,\n",
       "       -2.17365890e-02, -1.00005000e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_std[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76766ff1",
   "metadata": {},
   "source": [
    "## Reduciendo a 5 dimensiones, autoencoder\n",
    "\n",
    "Vamos a ver ahora como usamos un autoencoder para reducir nuestra dimensionalidad. Nuestro primer modelo es el más simple: una primera capa recibe un input de tamaño *num_dimensiones* y lo lleva a 5 neuronas. Luego esas 5 neuronas son \"decodificadas\" en un dataset con *num_dimensiones*.\n",
    "\n",
    "Como esto es un autoencoder, vamos a separar conceptualmente las capas en dos. \n",
    "- una capa encoder que reduce la dimensionalidad\n",
    "- una capa decoder que toma la dimensionalidad reducida y la devuelve a todas las dimensiones. \n",
    "\n",
    "La gracia es que podemos entrenar el autoencoder comparando los datos con los datos que fueron codificados y luego decodificados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0afef355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "219/219 [==============================] - 7s 17ms/step - loss: 0.8509 - val_loss: 5.4501\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - 3s 12ms/step - loss: 0.8328 - val_loss: 5.3974\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.8030 - val_loss: 5.3484\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.7740 - val_loss: 5.3145\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7462 - val_loss: 5.2807\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.7174 - val_loss: 5.2538\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6971 - val_loss: 5.2379\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6860 - val_loss: 5.2302\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6801 - val_loss: 5.2240\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.6768 - val_loss: 5.2200\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - 3s 13ms/step - loss: 0.6749 - val_loss: 5.2171\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6737 - val_loss: 5.2142\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6730 - val_loss: 5.2125\n",
      "Epoch 14/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.6724 - val_loss: 5.2107\n",
      "Epoch 15/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6719 - val_loss: 5.2099\n",
      "Epoch 16/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6716 - val_loss: 5.2082\n",
      "Epoch 17/100\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6713 - val_loss: 5.2074\n",
      "Epoch 18/100\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6710 - val_loss: 5.2062\n",
      "Epoch 19/100\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6708 - val_loss: 5.2051\n",
      "Epoch 20/100\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6707 - val_loss: 5.2042\n",
      "Epoch 21/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6705 - val_loss: 5.2037\n",
      "Epoch 22/100\n",
      "219/219 [==============================] - 3s 13ms/step - loss: 0.6704 - val_loss: 5.2026\n",
      "Epoch 23/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6703 - val_loss: 5.2024\n",
      "Epoch 24/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.6702 - val_loss: 5.2015\n",
      "Epoch 25/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6701 - val_loss: 5.2006\n",
      "Epoch 26/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6700 - val_loss: 5.1998\n",
      "Epoch 27/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6699 - val_loss: 5.2000\n",
      "Epoch 28/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6699 - val_loss: 5.1995\n",
      "Epoch 29/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6698 - val_loss: 5.1993\n",
      "Epoch 30/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6698 - val_loss: 5.1991\n",
      "Epoch 31/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.6697 - val_loss: 5.1989\n",
      "Epoch 32/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6697 - val_loss: 5.1983\n",
      "Epoch 33/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6696 - val_loss: 5.1975\n",
      "Epoch 34/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6696 - val_loss: 5.1975\n",
      "Epoch 35/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6696 - val_loss: 5.1971\n",
      "Epoch 36/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6695 - val_loss: 5.1967\n",
      "Epoch 37/100\n",
      "219/219 [==============================] - 3s 12ms/step - loss: 0.6695 - val_loss: 5.1966\n",
      "Epoch 38/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6695 - val_loss: 5.1965\n",
      "Epoch 39/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6695 - val_loss: 5.1960\n",
      "Epoch 40/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.6695 - val_loss: 5.1960\n",
      "Epoch 41/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.6694 - val_loss: 5.1961\n",
      "Epoch 42/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6694 - val_loss: 5.1960\n",
      "Epoch 43/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.6694 - val_loss: 5.1956\n",
      "Epoch 44/100\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6694 - val_loss: 5.1955\n",
      "Epoch 45/100\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6694 - val_loss: 5.1950\n",
      "Epoch 46/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6694 - val_loss: 5.1952\n",
      "Epoch 47/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6693 - val_loss: 5.1948\n",
      "Epoch 48/100\n",
      "219/219 [==============================] - 1s 7ms/step - loss: 0.6694 - val_loss: 5.1947\n",
      "Epoch 49/100\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6693 - val_loss: 5.1948\n",
      "Epoch 50/100\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 0.6693 - val_loss: 5.1948\n",
      "Epoch 51/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.6693 - val_loss: 5.1943\n",
      "Epoch 52/100\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 0.6693 - val_loss: 5.1945\n",
      "Epoch 53/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6693 - val_loss: 5.1939\n",
      "Epoch 54/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.6693 - val_loss: 5.1942\n",
      "Epoch 55/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.6693 - val_loss: 5.1940\n",
      "Epoch 56/100\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.6693 - val_loss: 5.1936\n",
      "Epoch 57/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6693 - val_loss: 5.1939\n",
      "Epoch 58/100\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.6693 - val_loss: 5.1932\n",
      "Epoch 59/100\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.6693 - val_loss: 5.1932\n",
      "Epoch 60/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6693 - val_loss: 5.1933\n",
      "Epoch 61/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.6692 - val_loss: 5.1935\n",
      "Epoch 62/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.6692 - val_loss: 5.1932\n",
      "Epoch 63/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.1934\n",
      "Epoch 64/100\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.6692 - val_loss: 5.1935\n",
      "Epoch 65/100\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.6692 - val_loss: 5.1931\n",
      "Epoch 66/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.6692 - val_loss: 5.1928\n",
      "Epoch 67/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6692 - val_loss: 5.1927\n",
      "Epoch 68/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.6692 - val_loss: 5.1928\n",
      "Epoch 69/100\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.6692 - val_loss: 5.1929\n",
      "Epoch 70/100\n",
      "219/219 [==============================] - 3s 15ms/step - loss: 0.6692 - val_loss: 5.1933\n",
      "Epoch 71/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.1932\n",
      "Epoch 72/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.1934\n",
      "Epoch 73/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6692 - val_loss: 5.1929\n",
      "Epoch 74/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.6692 - val_loss: 5.1930\n",
      "Epoch 75/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6692 - val_loss: 5.1931\n",
      "Epoch 76/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.6692 - val_loss: 5.1926\n",
      "Epoch 77/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.1928\n",
      "Epoch 78/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.1928\n",
      "Epoch 79/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6692 - val_loss: 5.1930\n",
      "Epoch 80/100\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.6692 - val_loss: 5.1929\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6692 - val_loss: 5.1926\n",
      "Epoch 82/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.1927\n",
      "Epoch 83/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6692 - val_loss: 5.1926\n",
      "Epoch 84/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6692 - val_loss: 5.1929\n",
      "Epoch 85/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6691 - val_loss: 5.1929\n",
      "Epoch 86/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6691 - val_loss: 5.1928\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "num_dimensiones = len(X_sample.columns)\n",
    "\n",
    "#### Sequential es un modelo en keras que simplemente va concatenando las capas, \n",
    "#### las neuronas del final de una capa son las del principio de otra\n",
    "\n",
    "#### keras.layers.Dense es una capa donde todas las neuronas están conectadas las unas a las otras\n",
    "\n",
    "#### armamos el encoder: el input conectado a 5 neuronas\n",
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(5, input_shape=[num_dimensiones]),\n",
    "])\n",
    "\n",
    "#### armamos el dencoder: esas 5 neuronas luego se conectan a todas las dimensiones del input \n",
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(num_dimensiones, input_shape=[5]),\n",
    "])\n",
    "\n",
    "#### y ahora concatenamos encoder y decoder\n",
    "autoencoder = keras.models.Sequential([encoder, decoder])\n",
    "\n",
    "#### compile lo deja listo para aprender. \n",
    "#### Para comparar el resultado del autoencoder y el dataset original \n",
    "#### usamos el clásico Mean Squared Error (a eso se le llama \"loss\")\n",
    "#### Para entrenar usamos StochasticGradientDescent, con un alfa de 0.1 \n",
    "\n",
    "autoencoder.compile(loss='mse', optimizer = keras.optimizers.SGD(lr=0.1))\n",
    "\n",
    "#### El entrenamiento toma los datos originales, los codifica/decodifica en vectores de la misma \n",
    "#### dimensión y los compara con los datos originales. \n",
    "#### Esto por 100 épocas, y para validar usamos el set de test. \n",
    "#### Para no perder tiempo, decidimos que debe parar si la métrica del MSE no mejora en 10 épocas. \n",
    "\n",
    "history = autoencoder.fit(X_train_std,X_train_std, epochs=100,validation_data=(X_test_std,X_test_std),\n",
    "                         callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "                         \n",
    "#### Finalmente, para codificar solo llamamos al método predict de la parte de encoder. \n",
    "X_auto = encoder.predict(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3cdb4831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.8925257 ,  4.7239914 ,  3.5750537 ,  1.0408279 , -4.456152  ],\n",
       "       [-7.8717313 ,  1.3005136 , -9.2127    , -3.6584291 ,  6.0989046 ],\n",
       "       [ 0.4235208 ,  4.9710464 , -4.7106285 , -3.69764   , -4.812702  ],\n",
       "       ...,\n",
       "       [-0.39876637, -1.8733788 ,  3.3826168 , -0.19737819, -5.264496  ],\n",
       "       [ 5.8387594 ,  0.31075642, -1.6960075 , -3.67774   , -3.1034296 ],\n",
       "       [-0.67234653, -8.578598  ,  3.2054076 , -8.661794  ,  8.426323  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee833bc0",
   "metadata": {},
   "source": [
    "## Probando cuál enfoque de reducción es mejor de forma indirecta: viendo qué dataset sirve más para clasificación. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada3c42",
   "metadata": {},
   "source": [
    "Pero como saber si esta codificación sirve de algo? \n",
    "Una posibilidad es probarlo de forma indirecta: ejecutando algún tipo de tarea de aprendizaje sobre estos datos, y comparando los resultados. \n",
    "Aquí, comparamos con la tarea de clasificar los números de MNIST usando RandomForest. Vamos a comparar el resultado de usar RandomForest en el todo, usando PCA, y usando estos modelos de autoencoders. \n",
    "\n",
    "Partimos por el original: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75c8049e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7909999999999999"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "np.mean(cross_val_score(clf, X_std, y_sample, cv=5, scoring='accuracy'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f89894",
   "metadata": {},
   "source": [
    "Ahora para el dataset transformado con autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "130b87ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6346999999999999"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(clf, X_auto, y_sample, cv=5, scoring='accuracy'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0478f28a",
   "metadata": {},
   "source": [
    "Y finalmente para la decomposicion con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c63c6e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "X_pca = pca.fit_transform(X_std) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e894a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6272"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(clf, X_pca, y_sample, cv=5, scoring='accuracy'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d221e39a",
   "metadata": {},
   "source": [
    "### Mejores autoencoders, ¿podremos acercarnos más al acuracy del dataset original?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f1631",
   "metadata": {},
   "source": [
    "Un primer enfoque puede ser agregar más capas a la red neuronal. Acá abajo agregamos dos capas adicionales tanto en el encoder con el decoder: pasamos de *num_dimensiones* a 200 neuronas, luego a 50 y luego a 5, y lo mismo para decodificar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd2a19e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\juanr\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "219/219 [==============================] - 5s 9ms/step - loss: 0.8542 - val_loss: 5.4894\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.7949 - val_loss: 5.3705\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.7385 - val_loss: 5.3186\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6969 - val_loss: 5.2838\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6789 - val_loss: 5.2655\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - 1s 7ms/step - loss: 0.6736 - val_loss: 5.2561\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6719 - val_loss: 5.2490\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6713 - val_loss: 5.2463\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6709 - val_loss: 5.2427\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6707 - val_loss: 5.2414\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6705 - val_loss: 5.2368\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6704 - val_loss: 5.2343\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6703 - val_loss: 5.2308\n",
      "Epoch 14/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6702 - val_loss: 5.2305\n",
      "Epoch 15/100\n",
      "219/219 [==============================] - 1s 7ms/step - loss: 0.6701 - val_loss: 5.2277\n",
      "Epoch 16/100\n",
      "219/219 [==============================] - 1s 7ms/step - loss: 0.6701 - val_loss: 5.2257\n",
      "Epoch 17/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6701 - val_loss: 5.2241\n",
      "Epoch 18/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6700 - val_loss: 5.2215\n",
      "Epoch 19/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6700 - val_loss: 5.2210\n",
      "Epoch 20/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6699 - val_loss: 5.2201\n",
      "Epoch 21/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6699 - val_loss: 5.2193\n",
      "Epoch 22/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6698 - val_loss: 5.2179\n",
      "Epoch 23/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6698 - val_loss: 5.2172\n",
      "Epoch 24/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6698 - val_loss: 5.2164\n",
      "Epoch 25/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6698 - val_loss: 5.2155\n",
      "Epoch 26/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6698 - val_loss: 5.2157\n",
      "Epoch 27/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6697 - val_loss: 5.2139\n",
      "Epoch 28/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6697 - val_loss: 5.2135\n",
      "Epoch 29/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6697 - val_loss: 5.2125\n",
      "Epoch 30/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6697 - val_loss: 5.2111\n",
      "Epoch 31/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6697 - val_loss: 5.2114\n",
      "Epoch 32/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6697 - val_loss: 5.2102\n",
      "Epoch 33/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6696 - val_loss: 5.2115\n",
      "Epoch 34/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6696 - val_loss: 5.2103\n",
      "Epoch 35/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6696 - val_loss: 5.2094\n",
      "Epoch 36/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6696 - val_loss: 5.2094\n",
      "Epoch 37/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6696 - val_loss: 5.2096\n",
      "Epoch 38/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6696 - val_loss: 5.2093\n",
      "Epoch 39/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6695 - val_loss: 5.2088\n",
      "Epoch 40/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6696 - val_loss: 5.2078\n",
      "Epoch 41/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6695 - val_loss: 5.2079\n",
      "Epoch 42/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6695 - val_loss: 5.2078\n",
      "Epoch 43/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6695 - val_loss: 5.2075\n",
      "Epoch 44/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6695 - val_loss: 5.2071\n",
      "Epoch 45/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6695 - val_loss: 5.2065\n",
      "Epoch 46/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6695 - val_loss: 5.2067\n",
      "Epoch 47/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6695 - val_loss: 5.2064\n",
      "Epoch 48/100\n",
      "219/219 [==============================] - 1s 7ms/step - loss: 0.6695 - val_loss: 5.2056\n",
      "Epoch 49/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6695 - val_loss: 5.2053\n",
      "Epoch 50/100\n",
      "219/219 [==============================] - 1s 7ms/step - loss: 0.6694 - val_loss: 5.2049\n",
      "Epoch 51/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6694 - val_loss: 5.2039\n",
      "Epoch 52/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6694 - val_loss: 5.2048\n",
      "Epoch 53/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6695 - val_loss: 5.2040\n",
      "Epoch 54/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6694 - val_loss: 5.2042\n",
      "Epoch 55/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6694 - val_loss: 5.2043\n",
      "Epoch 56/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6694 - val_loss: 5.2038\n",
      "Epoch 57/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6694 - val_loss: 5.2040\n",
      "Epoch 58/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6694 - val_loss: 5.2036\n",
      "Epoch 59/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6694 - val_loss: 5.2035\n",
      "Epoch 60/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6694 - val_loss: 5.2037\n",
      "Epoch 61/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6694 - val_loss: 5.2029\n",
      "Epoch 62/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6694 - val_loss: 5.2024\n",
      "Epoch 63/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6694 - val_loss: 5.2030\n",
      "Epoch 64/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6693 - val_loss: 5.2034\n",
      "Epoch 65/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6694 - val_loss: 5.2031\n",
      "Epoch 66/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6693 - val_loss: 5.2022\n",
      "Epoch 67/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6693 - val_loss: 5.2025\n",
      "Epoch 68/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6693 - val_loss: 5.2022\n",
      "Epoch 69/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6693 - val_loss: 5.2020\n",
      "Epoch 70/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6693 - val_loss: 5.2018\n",
      "Epoch 71/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6693 - val_loss: 5.2017\n",
      "Epoch 72/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.6693 - val_loss: 5.2018\n",
      "Epoch 73/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6693 - val_loss: 5.2017\n",
      "Epoch 74/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6693 - val_loss: 5.2012\n",
      "Epoch 75/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6693 - val_loss: 5.2015\n",
      "Epoch 76/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6693 - val_loss: 5.2015\n",
      "Epoch 77/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6693 - val_loss: 5.2010\n",
      "Epoch 78/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6693 - val_loss: 5.2012\n",
      "Epoch 79/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6693 - val_loss: 5.2005\n",
      "Epoch 80/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6693 - val_loss: 5.2007\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6693 - val_loss: 5.2009\n",
      "Epoch 82/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6693 - val_loss: 5.2009\n",
      "Epoch 83/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6693 - val_loss: 5.2008\n",
      "Epoch 84/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.2005\n",
      "Epoch 85/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.2005\n",
      "Epoch 86/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.2005\n",
      "Epoch 87/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.2002\n",
      "Epoch 88/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.2004\n",
      "Epoch 89/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.2001\n",
      "Epoch 90/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.2002\n",
      "Epoch 91/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.2002\n",
      "Epoch 92/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.1995\n",
      "Epoch 93/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.1994\n",
      "Epoch 94/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.2003\n",
      "Epoch 95/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.2001\n",
      "Epoch 96/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.1996\n",
      "Epoch 97/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.1998\n",
      "Epoch 98/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.2000\n",
      "Epoch 99/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.2000\n",
      "Epoch 100/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6692 - val_loss: 5.1997\n"
     ]
    }
   ],
   "source": [
    "### Modelo mas profundo\n",
    "\n",
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(200, input_shape=[num_dimensiones]),\n",
    "    keras.layers.Dense(50),\n",
    "    keras.layers.Dense(5),\n",
    "])\n",
    "\n",
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(50, input_shape=[5]),\n",
    "    keras.layers.Dense(200),\n",
    "    keras.layers.Dense(num_dimensiones),\n",
    "])\n",
    "\n",
    "autoencoder = keras.models.Sequential([encoder, decoder])\n",
    "autoencoder.compile(loss='mse', optimizer = keras.optimizers.SGD(lr=0.1))\n",
    "\n",
    "history = autoencoder.fit(X_train_std,X_train_std, epochs=100,validation_data=(X_test_std,X_test_std),\n",
    "                         callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "                         \n",
    "X_auto_deep = encoder.predict(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d09a4b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6303000000000001"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(clf, X_auto_deep, y_sample, cv=5, scoring='accuracy'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a2a35a",
   "metadata": {},
   "source": [
    "## Autoencoders con activación no lineal\n",
    "\n",
    "Un segundo enfoque, y lo más clásico en la práctica, es agregar una activación no lineal al final de cada capa. En este caso usamos la función Scaled Exponential Linear Unit (SELU). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf77b856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\juanr\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "219/219 [==============================] - 3s 9ms/step - loss: 0.8577 - val_loss: 5.3019\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.7759 - val_loss: 5.2559\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7361 - val_loss: 5.2233\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.7107 - val_loss: 5.2045\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6971 - val_loss: 5.1960\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6901 - val_loss: 5.1911\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6859 - val_loss: 5.1876\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6830 - val_loss: 5.1855\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6807 - val_loss: 5.1831\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6786 - val_loss: 5.1811\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6766 - val_loss: 5.1797\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6747 - val_loss: 5.1775\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6727 - val_loss: 5.1760\n",
      "Epoch 14/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6707 - val_loss: 5.1741\n",
      "Epoch 15/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6687 - val_loss: 5.1720\n",
      "Epoch 16/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6667 - val_loss: 5.1703\n",
      "Epoch 17/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6647 - val_loss: 5.1679\n",
      "Epoch 18/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6625 - val_loss: 5.1663\n",
      "Epoch 19/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6601 - val_loss: 5.1640\n",
      "Epoch 20/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6576 - val_loss: 5.1617\n",
      "Epoch 21/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6549 - val_loss: 5.1597\n",
      "Epoch 22/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6520 - val_loss: 5.1574\n",
      "Epoch 23/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6491 - val_loss: 5.1546\n",
      "Epoch 24/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6460 - val_loss: 5.1525\n",
      "Epoch 25/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6430 - val_loss: 5.1501\n",
      "Epoch 26/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6400 - val_loss: 5.1483\n",
      "Epoch 27/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6370 - val_loss: 5.1463\n",
      "Epoch 28/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6342 - val_loss: 5.1451\n",
      "Epoch 29/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6315 - val_loss: 5.1426\n",
      "Epoch 30/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6288 - val_loss: 5.1408\n",
      "Epoch 31/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6261 - val_loss: 5.1389\n",
      "Epoch 32/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6236 - val_loss: 5.1362\n",
      "Epoch 33/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6210 - val_loss: 5.1356\n",
      "Epoch 34/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6184 - val_loss: 5.1326\n",
      "Epoch 35/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6159 - val_loss: 5.1318\n",
      "Epoch 36/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6133 - val_loss: 5.1280\n",
      "Epoch 37/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6107 - val_loss: 5.1288\n",
      "Epoch 38/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6083 - val_loss: 5.1262\n",
      "Epoch 39/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6058 - val_loss: 5.1256\n",
      "Epoch 40/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6033 - val_loss: 5.1234\n",
      "Epoch 41/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.6009 - val_loss: 5.1237\n",
      "Epoch 42/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.5986 - val_loss: 5.1228\n",
      "Epoch 43/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.5962 - val_loss: 5.1195\n",
      "Epoch 44/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.5940 - val_loss: 5.1192\n",
      "Epoch 45/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5918 - val_loss: 5.1180\n",
      "Epoch 46/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5895 - val_loss: 5.1185\n",
      "Epoch 47/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5874 - val_loss: 5.1138\n",
      "Epoch 48/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5852 - val_loss: 5.1148\n",
      "Epoch 49/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5832 - val_loss: 5.1122\n",
      "Epoch 50/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5810 - val_loss: 5.1086\n",
      "Epoch 51/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5790 - val_loss: 5.1101\n",
      "Epoch 52/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5768 - val_loss: 5.1054\n",
      "Epoch 53/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5748 - val_loss: 5.1017\n",
      "Epoch 54/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5727 - val_loss: 5.1015\n",
      "Epoch 55/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5707 - val_loss: 5.1002\n",
      "Epoch 56/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5686 - val_loss: 5.0982\n",
      "Epoch 57/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.5665 - val_loss: 5.0933\n",
      "Epoch 58/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.5645 - val_loss: 5.0893\n",
      "Epoch 59/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.5623 - val_loss: 5.0883\n",
      "Epoch 60/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5603 - val_loss: 5.0862\n",
      "Epoch 61/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.5584 - val_loss: 5.0791\n",
      "Epoch 62/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.5563 - val_loss: 5.0842\n",
      "Epoch 63/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5541 - val_loss: 5.0796\n",
      "Epoch 64/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5523 - val_loss: 5.0738\n",
      "Epoch 65/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5503 - val_loss: 5.0745\n",
      "Epoch 66/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5484 - val_loss: 5.0696\n",
      "Epoch 67/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5464 - val_loss: 5.0677\n",
      "Epoch 68/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5442 - val_loss: 5.0680\n",
      "Epoch 69/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5423 - val_loss: 5.0642\n",
      "Epoch 70/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5406 - val_loss: 5.0659\n",
      "Epoch 71/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5387 - val_loss: 5.0550\n",
      "Epoch 72/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5368 - val_loss: 5.0556\n",
      "Epoch 73/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5347 - val_loss: 5.0503\n",
      "Epoch 74/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5328 - val_loss: 5.0480\n",
      "Epoch 75/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.5311 - val_loss: 5.0349\n",
      "Epoch 76/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5301 - val_loss: 5.0453\n",
      "Epoch 77/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5304 - val_loss: 5.0297\n",
      "Epoch 78/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.5259 - val_loss: 5.0294\n",
      "Epoch 79/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5237 - val_loss: 5.0187\n",
      "Epoch 80/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5221 - val_loss: 5.0287\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5203 - val_loss: 5.0030\n",
      "Epoch 82/100\n",
      "219/219 [==============================] - 1s 7ms/step - loss: 0.5186 - val_loss: 5.0182\n",
      "Epoch 83/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.5167 - val_loss: 5.0057\n",
      "Epoch 84/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5147 - val_loss: 4.9981\n",
      "Epoch 85/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.5135 - val_loss: 4.9977\n",
      "Epoch 86/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.5117 - val_loss: 5.0075\n",
      "Epoch 87/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.5100 - val_loss: 4.9844\n",
      "Epoch 88/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.5083 - val_loss: 4.9887\n",
      "Epoch 89/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.5074 - val_loss: 4.9771\n",
      "Epoch 90/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5057 - val_loss: 4.9764\n",
      "Epoch 91/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5035 - val_loss: 4.9640\n",
      "Epoch 92/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5025 - val_loss: 4.9692\n",
      "Epoch 93/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.5009 - val_loss: 4.9584\n",
      "Epoch 94/100\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 0.4991 - val_loss: 4.9574\n",
      "Epoch 95/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.4977 - val_loss: 4.9506\n",
      "Epoch 96/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.4962 - val_loss: 4.9411\n",
      "Epoch 97/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.4947 - val_loss: 4.9363\n",
      "Epoch 98/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.4937 - val_loss: 4.9352\n",
      "Epoch 99/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.4919 - val_loss: 4.9372\n",
      "Epoch 100/100\n",
      "219/219 [==============================] - 2s 8ms/step - loss: 0.4913 - val_loss: 4.9443\n"
     ]
    }
   ],
   "source": [
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(200, input_shape=[num_dimensiones],activation='selu'),\n",
    "    keras.layers.Dense(50,activation='selu'),\n",
    "    keras.layers.Dense(5,activation='selu'),\n",
    "])\n",
    "\n",
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.Dense(50, input_shape=[5],activation='selu'),\n",
    "    keras.layers.Dense(200,activation='selu'),\n",
    "    keras.layers.Dense(num_dimensiones,activation='selu'),\n",
    "])\n",
    "\n",
    "autoencoder = keras.models.Sequential([encoder, decoder])\n",
    "autoencoder.compile(loss='mse', optimizer = keras.optimizers.SGD(lr=0.1))\n",
    "\n",
    "history = autoencoder.fit(X_train_std,X_train_std, epochs=100,validation_data=(X_test_std,X_test_std),\n",
    "                         callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "                         \n",
    "X_auto_deep_relu = encoder.predict(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d3b7b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7063"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(clf, X_auto_deep_relu, y_sample, cv=5, scoring='accuracy'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7c5070",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "Experimenta con distintos modelos que reduzcan a dos dimensiones. Visualiza y compara como lo hicimos en la tarea 3, con las reducciones con PCA y T-SNE, tomando como referencia las clases reales (los números que representan cada fila). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bc47b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
